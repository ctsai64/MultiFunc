{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_channel_size, \n",
    "                 hidden_channel_size, \n",
    "                 sequence_length, \n",
    "                 num_heads = 4,\n",
    "                 drop_out = 0.1\n",
    "                ):\n",
    "        super(Transformer_Encoder, self).__init__()\n",
    "        \n",
    "        self.cov1d = nn.Conv1d(input_channel_size, hidden_channel_size, 2, stride=2, padding=0, padding_mode='zeros')\n",
    "        # Multihead attention layer\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=sequence_length, num_heads=num_heads,batch_first=True)\n",
    "        \n",
    "\n",
    "        # Feedforward layer\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(sequence_length, sequence_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(sequence_length, sequence_length)\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(sequence_length)\n",
    "        self.layer_norm2 = nn.LayerNorm(sequence_length)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multihead attention\n",
    "        x = self.cov1d(x)\n",
    "\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "\n",
    "\n",
    "        # Add and normalize\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feedforward\n",
    "        ff_output = self.feedforward(x)\n",
    "\n",
    "        # Add and normalize\n",
    "        x = self.layer_norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_channel_size, \n",
    "                 hidden_channel_size, \n",
    "                 sequence_length,\n",
    "                 num_heads=4,\n",
    "                 drop_out = 0.1\n",
    "                ):\n",
    "        super(Transformer_Decoder, self).__init__()\n",
    "        \n",
    "        self.covtrans1d = nn.ConvTranspose1d(input_channel_size, hidden_channel_size, 2, stride=2, padding=0, padding_mode='zeros')\n",
    "        # Multihead attention layer\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=sequence_length, num_heads=num_heads,batch_first=True)\n",
    "#        self.attention_2 = nn.MultiheadAttention(embed_dim=sequence_length, num_heads=num_heads,batch_first=True)\n",
    "        \n",
    "\n",
    "        # Feedforward layer\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(sequence_length, sequence_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(sequence_length, sequence_length)\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(sequence_length)\n",
    "#        self.layer_norm2 = nn.LayerNorm(sequence_length)\n",
    "        self.layer_norm3 = nn.LayerNorm(sequence_length)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multihead attention\n",
    "        \n",
    "        attn_output, _ = self.attention_1(x, x, x)\n",
    "\n",
    "        # Add and normalize\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # add multihead-attention to result from encoder\n",
    "#        attn_output, _ = self.attention_2(x,encoder_out,encoder_out)\n",
    "        \n",
    "        # Add and normalize\n",
    "#        x = self.layer_norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feedforward\n",
    "        ff_output = self.feedforward(x)\n",
    "        \n",
    "        # Add and normalize\n",
    "        x = self.layer_norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        x = self.covtrans1d(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sequence_length=96, \n",
    "                 input_channel=2, \n",
    "                 embedding_size = 16, \n",
    "                 cov1d_size=128,\n",
    "                 num_heads = 4,\n",
    "                 drop_out = 0.1\n",
    "                ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.cov1d = nn.Conv1d(input_channel, cov1d_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.cov1d_1 = nn.Conv1d(cov1d_size, 1, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        \n",
    "        sequence_length = int(sequence_length/2)\n",
    "        \n",
    "        \n",
    "        self.transformer_1 = Transformer_Encoder(cov1d_size, \n",
    "                                                 cov1d_size, \n",
    "                                                 sequence_length,\n",
    "                                                 num_heads,\n",
    "                                                 drop_out)\n",
    "        \n",
    "        sequence_length = int(sequence_length/2)\n",
    "        \n",
    "        \n",
    "        self.transformer_2 = Transformer_Encoder(cov1d_size, \n",
    "                                                 cov1d_size, \n",
    "                                                 sequence_length,\n",
    "                                                 num_heads,\n",
    "                                                 drop_out)\n",
    "        sequence_length = int(sequence_length/2)\n",
    "        \n",
    "        \n",
    "        self.transformer_3 = Transformer_Encoder(cov1d_size, \n",
    "                                                 cov1d_size, \n",
    "                                                 sequence_length,\n",
    "                                                 num_heads,\n",
    "                                                 drop_out)\n",
    "        \n",
    "        self.relu_1 = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.dense = nn.Linear(sequence_length, embedding_size)\n",
    "\n",
    "    #        self.sd = nn.Linear(32,8)\n",
    "    #        self.logspike = nn.Linear(32,8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.cov1d(x)\n",
    "        \n",
    "        enc_trans_1 = self.transformer_1(out)\n",
    "        \n",
    "        enc_trans_2 = self.transformer_2(enc_trans_1)\n",
    "        \n",
    "        enc_trans_3 = self.transformer_3(enc_trans_2)\n",
    "        \n",
    "        out = self.cov1d_1(enc_trans_3)\n",
    "        \n",
    "        out = out.squeeze(1)\n",
    "\n",
    "        out = self.dense(out)\n",
    "        \n",
    "        out = self.relu_1(out)\n",
    "        \n",
    "\n",
    "\n",
    "        return out, enc_trans_1, enc_trans_2, enc_trans_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sequence_length=12, \n",
    "                 input_channel=2, \n",
    "                 embedding_size = 16, \n",
    "                 cov1d_size=128,\n",
    "                 num_heads = 4,\n",
    "                 drop_out = 0.1\n",
    "                ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.cov1d = nn.Conv1d(1, cov1d_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.cov1d_1 = nn.Conv1d(cov1d_size, input_channel, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.dense = nn.Linear(embedding_size, sequence_length)\n",
    "        \n",
    "        \n",
    "        self.transformer_1 = Transformer_Decoder(cov1d_size, \n",
    "                                                 cov1d_size, \n",
    "                                                 sequence_length,\n",
    "                                                 num_heads,\n",
    "                                                 drop_out)\n",
    "        \n",
    "        sequence_length = int(sequence_length*2)\n",
    "        \n",
    "        \n",
    "        self.transformer_2 = Transformer_Decoder(cov1d_size, \n",
    "                                                 cov1d_size, \n",
    "                                                 sequence_length,\n",
    "                                                 num_heads,\n",
    "                                                 drop_out)\n",
    "        sequence_length = int(sequence_length*2)\n",
    "        \n",
    "        \n",
    "        self.transformer_3 = Transformer_Decoder(cov1d_size, \n",
    "                                                 cov1d_size, \n",
    "                                                 sequence_length,\n",
    "                                                 num_heads,\n",
    "                                                 drop_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "#        x = x.transpose(1, 2)\n",
    "        out = self.dense(x)\n",
    "    \n",
    "        out = out.unsqueeze(1)\n",
    "\n",
    "        out = self.cov1d(out)\n",
    "        \n",
    "        out = self.transformer_1(out)\n",
    "        \n",
    "        out = self.transformer_2(out)\n",
    "        \n",
    "        out = self.transformer_3(out)\n",
    "        \n",
    "        out = self.cov1d_1(out)\n",
    "        \n",
    "\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                 enc, \n",
    "                 dec, \n",
    "                 embedding_size = 16, \n",
    "                 num_vae=1,\n",
    "                 fix_channel = False,\n",
    "                 set_topK = True,\n",
    "                 topk = 1,\n",
    "                 channel_list = [0,1,2,3]\n",
    "                ):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "        self.num_vae = num_vae\n",
    "        self.mn  = nn.Linear(embedding_size, embedding_size)\n",
    "        self.var = nn.Linear(embedding_size,embedding_size)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.fix_channel = fix_channel\n",
    "        self.channel_list = channel_list\n",
    "        self.set_topK = set_topK\n",
    "        self.topk = topk\n",
    "        if self.num_vae == 2:\n",
    "            self.mn_after = nn.Linear(embedding_size,embedding_size)\n",
    "            self.var_after = nn.Linear(embedding_size,embedding_size)\n",
    "            self.relu_2 = nn.ReLU()\n",
    "#        self.embedding_layer = nn.Linear(embedding_size,embedding_size)\n",
    "        \n",
    "        if self.fix_channel:\n",
    "            to_zero = [e for e in range(embedding_size) if e not in self.channel_list]\n",
    "            self.zero_index = torch.tensor(to_zero,dtype = torch.int)\n",
    "                   \n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        return mu + eps*std\n",
    "    \n",
    "    \n",
    "    def specify_channel(self, embed_):\n",
    "        \n",
    "        embed_co = torch.clone(embed_)\n",
    "        embed_co[:,self.zero_index]=0\n",
    "        \n",
    "        return embed_co\n",
    "    \n",
    "    def set_K(self,embed_):\n",
    "        \n",
    "        embed_co = torch.clone(embed_)\n",
    "        for i in range(embed_.shape[0]):\n",
    "            to_zero = [e for e in range(embed_.shape[1]) if e not in torch.topk(embed_co[i],self.topk)[1]]\n",
    "            zero_index = torch.tensor(to_zero,dtype = torch.int)\n",
    "            embed_co[i,zero_index] = 0\n",
    "        \n",
    "        return embed_co\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, \n",
    "                x,\n",
    "               ):\n",
    "        # encode\n",
    "        encoder_out,enc_trans_1,enc_trans_2,enc_trans_3 = self.enc(x)\n",
    "        \n",
    "        # variational layer\n",
    "        mn_ = self.mn(encoder_out)\n",
    "        var_ = self.var(encoder_out)\n",
    "        h1 = self.reparameterize(mn_,var_)\n",
    "#        pre_embedding = self.embedding_layer(h1)\n",
    "        embedding = self.relu_1(h1)\n",
    "        \n",
    "        \n",
    "        # another VAE layer\n",
    "        if self.num_vae == 2:\n",
    "        # sample from the distribution having latent parameters z_mu, z_var\n",
    "        # reparameterize\n",
    "            mn_after = self.mn_after(embedding)\n",
    "            var_after = self.var_after(embedding)\n",
    "            h2 = self.reparameterize(mn_after,var_after)\n",
    "            h2 = self.relu_2(h2)\n",
    "            if self.fix_channel:\n",
    "                h2 = self.specify_channel(h2)\n",
    "            elif self.set_topK:\n",
    "                h2 = self.set_K(h2)\n",
    "            # decode\n",
    "            predicted = self.dec(h2)\n",
    "            \n",
    "            return predicted, embedding, mn_,var_, mn_after,var_after, h2\n",
    "        \n",
    "        if self.fix_channel:\n",
    "            \n",
    "            embedding = self.specify_channel(embedding)\n",
    "            \n",
    "        elif self.set_topK:\n",
    "            embedding = self.set_K(embedding)\n",
    "            \n",
    "        \n",
    "        predicted = self.dec(embedding)\n",
    "        \n",
    "        return predicted, embedding, mn_, var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_tensor(min=0, max=1, size=(1)):\n",
    "    out = (max - min) * torch.rand(size) + min\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Polynomial_func:\n",
    "    def __init__(self, x, range=[-1,1], degree = 3, size=(1,1), sample_size=100, device=\"cuda\"):\n",
    "        self.x = x\n",
    "        self.range = range\n",
    "        self.degree = degree\n",
    "        self.size = size\n",
    "        self.sample_size = sample_size\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self):\n",
    "        hold = []\n",
    "        for n in range(self.degree + 1):\n",
    "            p = rand_tensor(\n",
    "                min=self.range[0],\n",
    "                max=self.range[1],\n",
    "                size=(self.sample_size, self.size[0], self.size[1]),\n",
    "            )\n",
    "            hold.append(p)\n",
    "        params = torch.stack(hold)\n",
    "        params = torch.atleast_2d(params)\n",
    "        params = torch.transpose(params, 0, 1)\n",
    "        params = torch.transpose(params, 1, 2)\n",
    "        params = params.to(self.device)\n",
    "\n",
    "        x = (\n",
    "            torch.cat(params.shape[0] * [self.x])\n",
    "            .reshape(params.shape[0], -1)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        x = torch.transpose(x, 0, 1).to(self.device)\n",
    "        y = torch.zeros_like(x)\n",
    "\n",
    "        for d in range(self.degree+1):\n",
    "            coeffs = params[:,0,d,0]\n",
    "            y += coeffs*(x**d)\n",
    "        out = torch.zeros(\n",
    "                (params.shape[0], self.x.shape[0],\n",
    "                self.size[0], self.size[1])\n",
    "            ).to(self.device)            \n",
    "        out[:,:,0,0] = torch.transpose(y,0,1)\n",
    "        return torch.sum(out,dim=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(params, x, device):\n",
    "    x = x.to(device)\n",
    "    if params.size(1) > 1:\n",
    "        a = params[:, 0].type(torch.float).unsqueeze(1).to(device)\n",
    "        b = params[:, 1].type(torch.float).unsqueeze(1).to(device)\n",
    "        return a*x + b\n",
    "    else:\n",
    "        return torch.zeros_like(params)\n",
    "\n",
    "def quadratic_function(params, x, device):\n",
    "    params = params.to(device)\n",
    "    x = x.to(device)\n",
    "    y = torch.zeros((params.size(0),x.shape[0])).to(device)\n",
    "    if params.size(1) > 2:\n",
    "        for n in range(len(params[0])):\n",
    "            y += (params[:, n].type(torch.float).unsqueeze(1).to(device))*x**(3-n)\n",
    "        return y\n",
    "    else:\n",
    "        return torch.zeros_like(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "sequence_length = 100\n",
    "batch_size = 50\n",
    "data_range = [-1, 1]\n",
    "num_funcs = 2\n",
    "\n",
    "x_values = torch.linspace(data_range[0], data_range[1], sequence_length)\n",
    "\n",
    "linear = Polynomial_func(x=x_values, range=data_range, degree=1, sample_size=sample_size, device=device)\n",
    "lin_full = linear.generate()\n",
    "\n",
    "quad = Polynomial_func(x=x_values, range=data_range, degree=2, sample_size=sample_size, device=device)\n",
    "quad_full = quad.generate()\n",
    "\n",
    "all_data = DataLoader(torch.vstack([quad_full, lin_full]), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(output, target, x):\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    target = target.clone().detach().requires_grad_(True)\n",
    "    derivative_true = torch.autograd.grad(outputs=target, inputs=x, grad_outputs=torch.ones_like(target), allow_unused=True)[0]\n",
    "    if derivative_true is None:\n",
    "        derivative_true = torch.zeros_like(output)\n",
    "    derivative_pred = (torch.roll(output, shifts=-1, dims=0) - torch.roll(output, shifts=1, dims=0)) / (2 * 1e-8)\n",
    "    mse_function = torch.mean(torch.abs(target - output)**3)\n",
    "    mse_derivative = torch.mean(torch.abs(derivative_pred - derivative_true)**3)\n",
    "    return mse_function + mse_derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, x_data, input_channels, transformer, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data\n",
    "        self.input_channels = input_channels\n",
    "        self.params = sum(self.functions[1])\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.input_channels, out_channels=8, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(96),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(212, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.params),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, n=-1):\n",
    "        target = x.squeeze(dim=2)\n",
    "\n",
    "        x = torch.swapaxes(x, 1, 2).to(self.device)\n",
    "        x, _, _, _ = self.transformer(x)\n",
    "\n",
    "        x = self.hidden_x1(x)\n",
    "        xfc = torch.reshape(x, (n, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        x = torch.reshape(x, (n, 2, 128))\n",
    "        x = self.hidden_x2(x)\n",
    "\n",
    "        cnn_flat = self.flatten_layer(x)\n",
    "        encoded = torch.cat((cnn_flat, xfc), dim=1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "\n",
    "        for f in range(len(self.functions[0])):\n",
    "            output = self.functions[0][f](\n",
    "                embedding[:, start_index:start_index + self.functions[1][f]], \n",
    "                self.x_data, \n",
    "                device=self.device\n",
    "            ).to(self.device)\n",
    "            outputs.append(output)\n",
    "            loss = new_loss(output, target, x)\n",
    "            losses.append(loss)\n",
    "            start_index += self.functions[1][f]\n",
    "\n",
    "        outputs = torch.stack(outputs)\n",
    "        losses = torch.stack(losses)\n",
    "        best_index = torch.argmin(losses)\n",
    "        best_loss, best_out = losses[best_index], outputs[best_index]\n",
    "        \n",
    "        return best_out, best_loss, outputs, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                sequence_length_encoder= 96, \n",
    "                sequence_length_decoder = 12,\n",
    "                input_channel=2, \n",
    "                embedding_size = 16, \n",
    "                cov1d_size=128,\n",
    "                num_heads = 4,\n",
    "                drop_out = 0.1,\n",
    "                learning_rate = 3e-5,\n",
    "                num_vae = 1,\n",
    "                fix_channel = False,\n",
    "                set_topK = True,\n",
    "                topk = 2,\n",
    "                channel_list = None\n",
    "                ):\n",
    "    \n",
    "    encoder = Encoder(sequence_length=sequence_length_encoder, \n",
    "                      input_channel=input_channel, \n",
    "                      embedding_size = embedding_size, \n",
    "                      cov1d_size=cov1d_size,\n",
    "                      num_heads = num_heads,\n",
    "                      drop_out = drop_out\n",
    "                        ).to(device)\n",
    "    \n",
    "    decoder = Decoder(sequence_length=sequence_length_decoder, \n",
    "                      input_channel=input_channel, \n",
    "                      embedding_size = embedding_size, \n",
    "                      cov1d_size=cov1d_size,\n",
    "                      num_heads = num_heads,\n",
    "                      drop_out = drop_out\n",
    "                     ).to(device)\n",
    "    \n",
    "    vae = VAE(encoder,\n",
    "              decoder,\n",
    "              embedding_size = embedding_size,\n",
    "              num_vae = num_vae,\n",
    "              fix_channel = fix_channel,\n",
    "              set_topK = set_topK,\n",
    "              topk = topk,\n",
    "              channel_list = channel_list).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return vae, encoder, decoder, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[317], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer, encoder, decoder, optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcov1d_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# sequence_length_encoder=64,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#sequence_length_decoder=16,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#input_channel=2,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#embedding_size=16,\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#cov1d_size=128,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#num_heads=1\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(device, sequence_length_encoder, sequence_length_decoder, input_channel, embedding_size, cov1d_size, num_heads, drop_out, learning_rate, num_vae, fix_channel, set_topK, topk, channel_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      2\u001b[0m                 sequence_length_encoder\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m96\u001b[39m, \n\u001b[1;32m      3\u001b[0m                 sequence_length_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 channel_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     15\u001b[0m                 ):\n\u001b[0;32m---> 17\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minput_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                      \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcov1d_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov1d_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdrop_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdrop_out\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m Decoder(sequence_length\u001b[38;5;241m=\u001b[39msequence_length_decoder, \n\u001b[1;32m     26\u001b[0m                       input_channel\u001b[38;5;241m=\u001b[39minput_channel, \n\u001b[1;32m     27\u001b[0m                       embedding_size \u001b[38;5;241m=\u001b[39m embedding_size, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m                       drop_out \u001b[38;5;241m=\u001b[39m drop_out\n\u001b[1;32m     31\u001b[0m                      )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m     vae \u001b[38;5;241m=\u001b[39m VAE(encoder,\n\u001b[1;32m     34\u001b[0m               decoder,\n\u001b[1;32m     35\u001b[0m               embedding_size \u001b[38;5;241m=\u001b[39m embedding_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m               topk \u001b[38;5;241m=\u001b[39m topk,\n\u001b[1;32m     40\u001b[0m               channel_list \u001b[38;5;241m=\u001b[39m channel_list)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self, sequence_length, input_channel, embedding_size, cov1d_size, num_heads, drop_out)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_1 \u001b[38;5;241m=\u001b[39m Transformer_Encoder(cov1d_size, \n\u001b[1;32m     19\u001b[0m                                          cov1d_size, \n\u001b[1;32m     20\u001b[0m                                          sequence_length,\n\u001b[1;32m     21\u001b[0m                                          num_heads,\n\u001b[1;32m     22\u001b[0m                                          drop_out)\n\u001b[1;32m     24\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sequence_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_2 \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer_Encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov1d_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mcov1d_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mdrop_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sequence_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_3 \u001b[38;5;241m=\u001b[39m Transformer_Encoder(cov1d_size, \n\u001b[1;32m     36\u001b[0m                                          cov1d_size, \n\u001b[1;32m     37\u001b[0m                                          sequence_length,\n\u001b[1;32m     38\u001b[0m                                          num_heads,\n\u001b[1;32m     39\u001b[0m                                          drop_out)\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mTransformer_Encoder.__init__\u001b[0;34m(self, input_channel_size, hidden_channel_size, sequence_length, num_heads, drop_out)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov1d \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv1d(input_channel_size, hidden_channel_size, \u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, padding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Multihead attention layer\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Feedforward layer\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedforward \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     18\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(sequence_length, sequence_length),\n\u001b[1;32m     19\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     20\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(sequence_length, sequence_length)\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/multifunc/lib/python3.11/site-packages/torch/nn/modules/activation.py:1020\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;241m=\u001b[39m batch_first\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m embed_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_heads\n\u001b[0;32m-> 1020\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m*\u001b[39m num_heads \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim must be divisible by num_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qkv_same_embed_dim:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((embed_dim, embed_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "transformer, encoder, decoder, optimizer = build_model(\n",
    "    sequence_length_encoder=sequence_length,\n",
    "    sequence_length_decoder=16,\n",
    "    input_channel=1,\n",
    "    embedding_size=16,\n",
    "    cov1d_size=128,\n",
    "    num_heads=sequence_length/10,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "    # sequence_length_encoder=64,\n",
    "    #sequence_length_decoder=16,\n",
    "    #input_channel=2,\n",
    "    #embedding_size=16,\n",
    "    #cov1d_size=128,\n",
    "    #num_heads=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sequence_length_encoder= 96, \n",
    "                sequence_length_decoder = 12,\n",
    "                input_channel=2, \n",
    "                embedding_size = 16, \n",
    "                cov1d_size=128,\n",
    "                num_heads = 4,\n",
    "                drop_out = 0.1,\n",
    "                learning_rate = 3e-5,\n",
    "                num_vae = 1,\n",
    "                fix_channel = False,\n",
    "                set_topK = True,\n",
    "                topk = 2,\n",
    "                channel_list = None'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multi_Func(functions=[[linear_function, quadratic_function],[2,3]], x_data=x_values, input_channels=1, transformer=transformer, device=device).to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/20, loss = 0.57536858\n",
      "--- 3.2550032138824463 seconds ---\n",
      "epoch : 1/20, loss = 0.40674062\n",
      "--- 3.2124452590942383 seconds ---\n",
      "epoch : 2/20, loss = 0.39138499\n",
      "--- 3.0711216926574707 seconds ---\n",
      "epoch : 3/20, loss = 0.38512788\n",
      "--- 3.1656503677368164 seconds ---\n",
      "epoch : 4/20, loss = 0.35064402\n",
      "--- 3.1270952224731445 seconds ---\n",
      "epoch : 5/20, loss = 0.33964737\n",
      "--- 3.0097591876983643 seconds ---\n",
      "epoch : 6/20, loss = 0.31081852\n",
      "--- 3.1394801139831543 seconds ---\n",
      "epoch : 7/20, loss = 0.29309857\n",
      "--- 3.143803358078003 seconds ---\n",
      "epoch : 8/20, loss = 0.28606998\n",
      "--- 3.034482479095459 seconds ---\n",
      "epoch : 9/20, loss = 0.29042843\n",
      "--- 3.1269164085388184 seconds ---\n",
      "epoch : 10/20, loss = 0.28304868\n",
      "--- 3.2351672649383545 seconds ---\n",
      "epoch : 11/20, loss = 0.27314179\n",
      "--- 3.1502151489257812 seconds ---\n",
      "epoch : 12/20, loss = 0.26378138\n",
      "--- 3.0508413314819336 seconds ---\n",
      "epoch : 13/20, loss = 0.29373921\n",
      "--- 3.148566246032715 seconds ---\n",
      "epoch : 14/20, loss = 0.27281673\n",
      "--- 3.1730263233184814 seconds ---\n",
      "epoch : 15/20, loss = 0.28389279\n",
      "--- 3.1104772090911865 seconds ---\n",
      "epoch : 16/20, loss = 0.29023835\n",
      "--- 3.053762674331665 seconds ---\n",
      "epoch : 17/20, loss = 0.28362536\n",
      "--- 3.1999571323394775 seconds ---\n",
      "epoch : 18/20, loss = 0.28130652\n",
      "--- 3.231290340423584 seconds ---\n",
      "epoch : 19/20, loss = 0.27432028\n",
      "--- 3.0295557975769043 seconds ---\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    model.train()\n",
    "    \n",
    "    for train_batch in all_data:\n",
    "        train_batch = train_batch.to(device)\n",
    "        target_batch = train_batch.squeeze(2).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        best_out,_,_,_ = model(train_batch)\n",
    "        loss = loss_func(best_out,target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "    scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    print(f\"epoch : {epoch}/{epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sample_size = 100\n",
    "\n",
    "t_x_values = torch.linspace(-1, 1, 1000)\n",
    "t_linear = Polynomial_func(x=x_values, range=[-1,1], degree=1, sample_size=t_sample_size, device=device)\n",
    "t_quad = Polynomial_func(x=x_values, range=[-1,1], degree=2, sample_size=t_sample_size, device=device)\n",
    "\n",
    "t_lin_full = t_linear.generate()\n",
    "t_quad_full = t_quad.generate()\n",
    "\n",
    "\n",
    "t_data = torch.vstack([t_lin_full, t_quad_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval\n",
    "best_out, best_loss, outputs, losses = model(t_data)\n",
    "rand = np.random.randint(0, t_sample_size*num_funcs)\n",
    "plt.plot(best_out[rand].detach().cpu().numpy(), \"r\", label='Predicted')\n",
    "plt.plot(t_data[rand].detach().cpu().numpy(), \"b\", label='True')\n",
    "#plt.plot(outputs[0][rand].detach().cpu().numpy(), \"g\", label='lin')\n",
    "#plt.plot(outputs[1][rand].detach().cpu().numpy(), \"g\", label='quad')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multifunc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
