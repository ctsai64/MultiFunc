{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_tensor(min=0, max=1, size=(1)):\n",
    "    out = (max - min) * torch.rand(size) + min\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, x_data, input_channels, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions  # [[function, (...)], [# params, (...)]]\n",
    "        self.x_data = x_data\n",
    "        self.input_channels = input_channels\n",
    "        self.params = sum(self.functions[1])\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.input_channels, out_channels=8, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128), #28, 16\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64), #16, 8\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.params), #8\n",
    "        )\n",
    "\n",
    "    def forward(self, x, n=-1):\n",
    "        target = x.squeeze(dim=2)\n",
    "        x = torch.swapaxes(x, 1, 2).to(self.device)\n",
    "        x = self.hidden_x1(x)\n",
    "        xfc = torch.reshape(x, (n, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        x = torch.reshape(x, (n, 2, 128))\n",
    "        x = self.hidden_x2(x)\n",
    "        cnn_flat = self.flatten_layer(x)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        loss_func = nn.MSELoss()\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        \n",
    "        for f in range(len(self.functions[0])):\n",
    "            output = self.functions[0][f](\n",
    "                embedding[:, start_index:start_index+self.functions[1][f]], \n",
    "                self.x_data, \n",
    "                device=self.device\n",
    "            ).to(device)\n",
    "            outputs.append(output)\n",
    "            loss = loss_func(output, target)\n",
    "            losses.append(loss)\n",
    "            start_index += self.functions[1][f]        \n",
    "        best_index = torch.argmin(torch.tensor(losses))\n",
    "        best_func = self.functions[0][best_index]\n",
    "        best_loss, best_out = losses[best_index], outputs[best_index]\n",
    "\n",
    "        return best_out, best_loss, best_func, outputs, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Polynomial_func:\n",
    "    def __init__(self, x, range=[-1,1], degree = 3, size=(1,1), sample_size=100, device=\"cuda\"):\n",
    "        self.x = x\n",
    "        self.range = range\n",
    "        self.degree = degree\n",
    "        self.size = size\n",
    "        self.sample_size = sample_size\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self):\n",
    "        hold = []\n",
    "        for n in range(self.degree + 1):\n",
    "            p = rand_tensor(\n",
    "                min=self.range[0],\n",
    "                max=self.range[1],\n",
    "                size=(self.sample_size, self.size[0], self.size[1]),\n",
    "            )\n",
    "            hold.append(p)\n",
    "        params = torch.stack(hold)\n",
    "        params = torch.atleast_2d(params)\n",
    "        params = torch.transpose(params, 0, 1)\n",
    "        params = torch.transpose(params, 1, 2)\n",
    "        params = params.to(self.device)\n",
    "\n",
    "        x = (\n",
    "            torch.cat(params.shape[0] * [self.x])\n",
    "            .reshape(params.shape[0], -1)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        x = torch.transpose(x, 0, 1).to(self.device)\n",
    "        y = torch.zeros_like(x)\n",
    "\n",
    "        for d in range(self.degree+1):\n",
    "            coeffs = params[:,0,d,0]\n",
    "            y += coeffs*(x**d)\n",
    "        out = torch.zeros(\n",
    "                (params.shape[0], self.x.shape[0],\n",
    "                self.size[0], self.size[1])\n",
    "            ).to(self.device)            \n",
    "        out[:,:,0,0] = torch.transpose(y,0,1)\n",
    "        return (torch.sum(out,dim=3), out), params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sin_func:\n",
    "    def __init__(self, x, amp=[0, 1], phase=[0, 1], frequency=[0, 1], size=(1, 1), batch_size=1000, device=\"cuda\"):\n",
    "        self.x = x\n",
    "        self.amp_range = amp\n",
    "        self.phase_range = phase\n",
    "        self.frequency_range = frequency\n",
    "        self.size = size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self):\n",
    "        hold = []\n",
    "        for param_range in [self.amp_range, self.phase_range, self.frequency_range]:\n",
    "            p = torch.uniform(torch.tensor(param_range[0]), torch.tensor(param_range[1]), (self.batch_size, self.size[0], self.size[1]))\n",
    "            hold.append(p)\n",
    "        \n",
    "        params = torch.stack(hold)\n",
    "        params = torch.atleast_2d(params)\n",
    "        params = torch.transpose(params, 0, 1)\n",
    "        params = torch.transpose(params, 1, 2)\n",
    "        params = params.to(self.device)\n",
    "\n",
    "        x = (\n",
    "            torch.cat(params.shape[0] * [self.x])\n",
    "            .reshape(params.shape[0], -1)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        x = torch.transpose(x, 0, 1).to(self.device)\n",
    "        \n",
    "        out = torch.zeros(\n",
    "            (params.shape[0], self.x.shape[0], self.size[0], self.size[1])\n",
    "        ).to(self.device)\n",
    "        \n",
    "        for i in range(self.size[1]):\n",
    "            amp = params[:, 0, i]\n",
    "            phase = params[:, 1, i]\n",
    "            frequency = params[:, 2, i]\n",
    "            \n",
    "            _out = amp * torch.sin(2 * torch.tensor(np.pi).to(self.device) * frequency[:, None] * x + phase[:, None])\n",
    "            out[:, :, 0, i] = torch.transpose(_out, 0, 1)\n",
    "\n",
    "        return (torch.sum(out, dim=3), out), params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(params, x, device):\n",
    "    x = x.to(device)\n",
    "    if params.size(1) > 1:\n",
    "        a = params[:, 0].type(torch.float).unsqueeze(1).to(device)\n",
    "        b = params[:, 1].type(torch.float).unsqueeze(1).to(device)\n",
    "        return a*x + b\n",
    "    else:\n",
    "        return torch.zeros_like(params)\n",
    "\n",
    "def quadratic_function(params, x, device):\n",
    "    params = params.to(device)\n",
    "    x = x.to(device)\n",
    "    y = torch.zeros((params.size(0),x.shape[0])).to(device)\n",
    "    if params.size(1) > 2:\n",
    "        for n in range(len(params[0])):\n",
    "            y += (params[:, n].type(torch.float).unsqueeze(1).to(device))*x**(3-n)\n",
    "        return y\n",
    "    else:\n",
    "        return torch.zeros_like(params)\n",
    "\n",
    "def cubic_function(params, x, device):\n",
    "    params = params.to(device)\n",
    "    x = x.to(device)\n",
    "    y = torch.zeros((params.size(0),x.shape[0])).to(device)\n",
    "    if params.size(1) > 3:\n",
    "        for n in range(len(params[0])):\n",
    "            y += (params[:, n].type(torch.float).unsqueeze(1).to(device))*x**(3-n)\n",
    "        return y\n",
    "    else:\n",
    "        return torch.zeros_like(params)\n",
    "    \n",
    "def sine_function(params, x, device):\n",
    "    x = x.to(device)\n",
    "    if params.size(1) == 3:\n",
    "        amplitude = params[:, 0].type(torch.float).unsqueeze(1).to(device)\n",
    "        frequency = params[:, 1].type(torch.float).unsqueeze(1).to(device)\n",
    "        phase = params[:, 2].type(torch.float).unsqueeze(1).to(device)\n",
    "        return amplitude * torch.sin(2 * torch.pi * frequency * x + phase)\n",
    "    else:\n",
    "        return torch.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
